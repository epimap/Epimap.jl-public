import StatsFuns: normlogpdf
function ð’©â‚Š(Î¼, Ïƒ)
    T = Î¼ isa AbstractFloat ? typeof(Î¼) : typeof(float(Î¼))
    truncated(Normal(Î¼, Ïƒ), zero(T), T(Inf))
end

"""
    lowerboundednormlogpdf(Î¼, Ïƒ, x, lb)

Computes the logpdf of a lower-bounded normal.

## Notes
Taking the derivatives of `StatsFuns.normcdf(Î¼, Ïƒ, Inf)`
results in `Inf` in gradients, and therefore `truncatednormlogpdf` requires
if-statements to check if the bounds are finite.
`lowerobundednormlogpdf` is therefore useful to avoid these if-statements.
"""
function lowerboundednormlogpdf(Î¼, Ïƒ, x, lb)
    lcdf = isinf(lb) ? zero(lb) : StatsFuns.normcdf((lb - Î¼) / Ïƒ)
    logtp = log(1 - lcdf)
    return StatsFuns.normlogpdf((x - Î¼) / Ïƒ) - log(Ïƒ) - logtp
end

# HACK: The way `zval` is computed within `StatsFuns.normlogdf` and `StatsFuns.normcdf`
# causes type-instabilities for AD-frameworks.
function halfnormlogpdf(Î¼, Ïƒ, x)
    # Just compute the zval instead of messing around with types to `zero(T)`.
    logtp = log(1 - StatsFuns.normcdf(-Î¼ / Ïƒ))
    return StatsFuns.normlogpdf((x - Î¼) / Ïƒ) - log(Ïƒ) - logtp
end


"""
    truncatednormlogpdf(Î¼, Ïƒ, x, lb, ub)

Computes the logpdf of a truncated normal.
"""
function truncatednormlogpdf(Î¼, Ïƒ, x, lb, ub)
    lcdf = isinf(lb) ? zero(lb) : StatsFuns.normcdf(Î¼, Ïƒ, lb)
    ucdf = isinf(ub) ? one(ub) : StatsFuns.normcdf(Î¼, Ïƒ, ub)
    logtp = log(ucdf - lcdf)
    return StatsFuns.normlogpdf((x - Î¼) / Ïƒ) - log(Ïƒ) - logtp
end


"""
    NegativeBinomial2(Î¼, Ï•)

Mean-variance parameterization of `NegativeBinomial`.

## Derivation
`NegativeBinomial` from `Distributions.jl` is parameterized following [1]. With the parameterization in [2], we can solve
for `r` (`n` in [1]) and `p` by matching the mean and the variance given in `Î¼` and `Ï•`.
We have the following two equations
(1) Î¼ = r (1 - p) / p
(2) Î¼ + Î¼^2 / Ï• = r (1 - p) / p^2
Substituting (1) into the RHS of (2):
  Î¼ + (Î¼^2 / Ï•) = Î¼ / p
âŸ¹ 1 + (Î¼ / Ï•) = 1 / p
âŸ¹ p = 1 / (1 + Î¼ / Ï•)
âŸ¹ p = (1 / (1 + Î¼ / Ï•)
Then in (1) we have
  Î¼ = r (1 - (1 / 1 + Î¼ / Ï•)) * (1 + Î¼ / Ï•)
âŸ¹ Î¼ = r ((1 + Î¼ / Ï•) - 1)
âŸ¹ r = Ï•
Hence, the resulting map is `(Î¼, Ï•) â†¦ NegativeBinomial(Ï•, 1 / (1 + Î¼ / Ï•))`.

## References
[1] https://reference.wolfram.com/language/ref/NegativeBinomialDistribution.html
[2] https://mc-stan.org/docs/2_20/functions-reference/nbalt.html
"""
function NegativeBinomial2(Î¼, Ï•)
    p = 1 / (1 + Î¼ / Ï•)
    r = Ï•

    return NegativeBinomial(r, p)
end

NegativeBinomial3(Î¼, Ï•) = NegativeBinomial2(Î¼, Î¼ / Ï•)

@inline nbinomlogpdf3(Î¼, Ï•, k) = nbinomlogpdf2(Î¼, Î¼ / Ï•, k)
@inline function nbinomlogpdf2(Î¼, Ï•, k)
    p = 1 / (1 + Î¼ / Ï•)
    r = Ï•

    return nbinomlogpdf(p, r, k)
end

"""
    nbinomlogpdf(p, r, k)

Julia implementation of `StatsFuns.nbinomlogpdf`.

## Notes
- Note: `SpecialFunctions.logbeta(a::Real, b::Int)` will result in a call to
  `SpecialFunctions.loggamma(b::Int)` which returns `Float64`. Therefore,
  to preserve floating point precision, `k` needs to be converted into float.

## Examples
```jldoctest
julia> using Epimap

julia> p = 0.5; r = 10; k = 5;

julia> Epimap.nbinomlogpdf(p, r, k) â‰ˆ logpdf(NegativeBinomial(r, p), k)
true
```

"""
@inline function nbinomlogpdf(p, r, k)
    r_ = r * log(p) + k * log1p(-p)
    return r_ - log(k + r) - SpecialFunctions.logbeta(r, k + 1)
end

"""
    GammaMeanCv(mean, cv)

Mean-variance-coefficient parameterization of `Gamma`.

## References
- https://www.rdocumentation.org/packages/EnvStats/versions/2.3.1/topics/GammaAlt
"""
function GammaMeanCv(mean, cv)
    k = cv^(-2)
    Î¸ = mean / k
    return Gamma(k, Î¸)
end


@doc raw"""
    AR1(num_times, Î±, Î¼, Ïƒ)

1-th order Autoregressive model for `num_times` steps and autocorrelation `Î±`.

Specifically, it defines the process
```math
\begin{align*}
\xi_t & \sim \mathcal{N}(0, 1) & \quad \forall t = 1, \dots, T \\
X_1 &= \xi_1 \\
X_t &= \alpha X_{t - 1} + \sqrt{1 - \alpha^2} \xi_t & \quad \forall t = 2, \dots, T
\end{align*}
```

## Arguments
- `num_times::Int`: number of time steps for the model
- `Î±`: autocorrelation for.
"""
struct AR1{T1, T2, T3} <: ContinuousMultivariateDistribution
    num_times::Int
    alpha::T1
    mu::T2
    sigma::T3
end

Base.size(ar::AR1) = (ar.num_times, )
Base.length(ar::AR1) = ar.num_times
Base.eltype(::AR1{T1, T2, T3}) where {T1, T2, T3} = promote_type(
    eltype(T1), eltype(T2), eltype(T3)
)

Bijectors.bijector(::AR1) = Bijectors.Identity{1}()

function Distributions.rand(rng::Random.AbstractRNG, ar::AR1)    
    # Sample
    Î¾ = randn(rng, length(ar))
    
    # Pre-compute the scaling factor
    Î´ = sqrt(1 - ar.alpha^2)
    
    # Pre-allocate
    x = similar(Î¾)
    
    # Compute
    x[1] = Î¾[1]
    for t = 2:length(ar)
        x[t] = ar.alpha * x[t - 1] + Î´ * Î¾[t]
    end
    
    # Î¼ (mean of the process) + Ïƒ * xâ‚œ (scale everything)
    return @. ar.mu + ar.sigma * x
end

function Distributions.logpdf(ar::AR1, y::AbstractVector{<:Real})
    Î´ = sqrt(1 - ar.alpha^2)
    
    # Recover `x`
    x = @. (y / ar.sigma) - ar.mu
    
    # Compute the mean for x[2:end]
    Î¼ = ar.alpha .* x[1:end - 1]
    
    return StatsFuns.normlogpdf(x[1]) + sum(StatsFuns.normlogpdf.((x[2:end] .- Î¼) ./ Î´))
end
